# -*- coding: utf-8 -*-
"""HW2_Bhattacharya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pzOqhWN408XdDMROMAvzSFENmEEJJaND
"""

from google.colab import files

uploaded = files.upload()

# Let's import libraries we will use
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

#file = open('1644871288_9762487_cleveland-train.csv')
df = pd.read_csv("1644871288_9762487_cleveland-train.csv")
# First 5 rows of our data
df.head()

df=df.rename(columns=({"heartdisease::category|-1|1":"final"}))
df["final"] = df.final.replace(-1,0)
df.final.value_counts()

countNoDisease = len(df[df.final == -1])
countHaveDisease = len(df[df.final == 1])
print("Percentage of Patients Haven't Heart Disease: {:.2f}%".format((countNoDisease / (len(df.final))*100)))
print("Percentage of Patients Have Heart Disease: {:.2f}%".format((countHaveDisease / (len(df.final))*100)))

y = df.final.values
x_data = df.drop(['final'], axis = 1)

y

# Normalize
# x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# transform data

x = scaler.fit_transform(x_data)

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)
#transpose matrices
x_train = x_train.T
y_train = y_train.T
x_test = x_test.T
y_test = y_test.T
x_train.shape

def initialize(dimension):
    
    weight = np.full((dimension,1),0)
    bias = 0.0
    return weight,bias

def sigmoid(z):
    y_head = 1/(1+ np.exp(-z))
    return y_head

#%%time
def gradientCostFunction(weight,bias,x_train,y_train):
    yHead = sigmoid(np.dot(weight.T,x_train) + bias)
    dW = np.dot(x_train,((yHead-y_train).T))/x_train.shape[1]
    dB = np.sum(yHead-y_train)/x_train.shape[1]
    loss = -(y_train*np.log(yHead) + (1-y_train)*np.log(1-yHead))
    cost = np.sum(loss) / x_train.shape[1]
    gradients = {"Derivative_Weight" : dW, "Derivative_Bias" : dB}
    return cost,gradients

def predictFunction(weight,bias,x_test):
    z = np.dot(weight.T,x_test) + bias
    yHead = sigmoid(z)
    y_pred = np.zeros((1,x_test.shape[1]))
    
    for i in range(yHead.shape[1]):
        if yHead[0,i] <= 0.5:
            y_pred[0,i] = 0
        else:
            y_pred[0,i] = 1
    return y_pred

def weightUpdate(weight,bias,x_train,y_train,learningRate,iteration):
    costList = []
    index = []
    for i in range(iteration):
        cost,gradients = gradientCostFunction(weight,bias,x_train,y_train)
        if(np.all(gradients["Derivative_Weight"]<0.001) and gradients["Derivative_Bias"]<0.001):
          break
        weight = weight - learningRate * gradients["Derivative_Weight"]
        bias = bias - learningRate * gradients["Derivative_Bias"]
        costList.append(cost)
        index.append(i)
        parameters = {"weight": weight,"bias": bias}

    print("iteration:",iteration)
    print("cost:",cost)
    plt.plot(index,costList)
    plt.xlabel("Number of Iteration")
    plt.ylabel("Cost")
    plt.show()
    return parameters, gradients

def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):
    dimension = x_train.shape[0]
    weight,bias = initialize(dimension)
    
    parameters, gradients = weightUpdate(weight,bias,x_train,y_train,learningRate,iteration)
    y_pred = predictFunction(parameters["weight"],parameters["bias"],x_test)
    print("Manual Test Accuracy: {:.2f}%".format((100 - np.mean(np.abs(y_pred - y_test))*100)/100*100))
    return parameters["weight"],parameters["bias"], y_pred

preditcted_weight, predicted_bias, y_pred=logistic_regression(x_train,y_train,x_test,y_test,0.00001,100000)

lr = LogisticRegression()
lr.fit(x_train.T,y_train.T)
print("Test Accuracy {:.2f}%".format(lr.score(x_test.T,y_test.T)*100))

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred.T))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,y_pred.T))
sns.heatmap(confusion_matrix(y_test,y_pred.T),annot=True)

td = pd.read_csv("1644871288_9775174_cleveland-test.csv")
# First 5 rows of our data
td.head()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# transform data

x_testdata= scaler.fit_transform(td)

#transpose matrices
x_test_data = x_testdata.T
x_test_data.shape

def predictTestData(weight,bias,x_test_data):
    count=[]
    z = np.dot(weight.T,x_test_data) + bias
    y_head = sigmoid(z)
    #y_prediction = np.zeros((1,x_test_data.shape[1]))
    
    for i in range(y_head.shape[1]):
        if y_head[0,i] <= 0.5:
            count.append(-1)
        else:
            count.append(1)
    return pd.DataFrame(count)

preditcted_weight, predicted_bias

y_pred_test = predictTestData(preditcted_weight, predicted_bias, x_test_data)

output = open('./outputAssing2.txt', 'w')

output.write(y_pred_test.to_string(header=False, index=False))
output.close()

"""# New Section"""