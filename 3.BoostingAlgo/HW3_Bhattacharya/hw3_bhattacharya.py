# -*- coding: utf-8 -*-
"""HW3_Bhattacharya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cVwslMKX3VnHpx0E08D43QBt6jrnX8g0
"""

from google.colab import files

uploaded = files.upload()

# Let's import libraries we will use
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

#file = open('1644871288_9762487_cleveland-train.csv')
df = pd.read_csv("1646102559_507402_train35.txt", header=None)
# First 5 rows of our data
df.head()
df.dropna

y = df.get(0)
x_data = df.drop([0], axis = 1)

y

pd.DataFrame(y)

y

set(y)

y= y.replace(5,1)
y= y.replace(3,-1)

y

set(y)

x_train, x_test, y_train, y_test = train_test_split(x_data,y,test_size = 0.2,random_state=0)

x_train

y_test

class AdaBoost:
    """ AdaBoost classifier  """

    def __init__(self):
        self.stumps = None
        self.stump_wts = None
        self.errors = None
        self.sample_wts = None

    def _check_X_y(self, X, y):
        assert set(y) == {-1, 1}
        return X, y

def gini_impurity_tot(a=0, b=0, c=0, d=0):
        tot_elements = a + b + c + d
        gini1 = 1 - np.square(a/(a+b)) - np.square(b/(a+b))
        gini2 = 1 - np.square(c/(c+d)) - np.square(d/(c+d))
        tot_gini = ((a+b)/tot_elements) * gini1 + ((c+d)/tot_elements) * gini2
        return tot_gini

def cal_Gini_For_Col(u_col, v):
  mAvg = u_col.max() + u_col.min()
  mAvg /=2
  gini_left = 0
  gini_right = 0
  a,b,c,d = 0,0,0,0
  for u_val, v_val in  zip(u_col, v):
    if (u_val < mAvg):
      if(int(v_val) == 1):
        a+=1
      else:
        b+=1
    else:
      if(int(u_val) == 1):
        c+=1
      else:
        d+=1
  return gini_impurity_tot(a,b,c,d), a, b, c, d

def stum_descision( X, y):
  mini_Gini = 3.00000000
  mini_Gini_Index = 0.000000
  ma, mb, mc, md = 0,0,0,0
  for i in range(1, 256):
    cal_Gini, a, b , c, d = cal_Gini_For_Col(X[i], y)
    if(mini_Gini > cal_Gini):
      mini_Gini = cal_Gini
      mini_Gini_Index = i
      ma, mb, mc, md = a, b , c, d
  return mini_Gini, mini_Gini_Index

print("Minimum Gini Index and respective column number:", stum_descision(x_train, y_train) )

from sklearn.tree import DecisionTreeClassifier

def fit(self, X: np.ndarray, y: np.ndarray, iters: int):


    X, y = self._check_X_y(X, y)
    n = X.shape[0]
    key= []
    self.sample_wts = np.zeros(shape=(iters, n))
    self.stumps = np.zeros(shape=iters, dtype=object)
    self.stump_wts = np.zeros(shape=iters)
    self.errors = np.zeros(shape=iters)
    self.sample_wts[0] = np.ones(shape=n) / n

    for x in range(iters):
        curr_sample_wts = self.sample_wts[x]
        stump = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)
        stump = stump.fit(X, y, sample_weight=curr_sample_wts)

        stump_pred = stump.predict(X)
        error = curr_sample_wts[(stump_pred != y)].sum()
        stump_wt = np.log((1 - error) / error) / 2

        new_sample_wts = (
            curr_sample_wts * np.exp(-stump_wt * y * stump_pred)
        )
        
        new_sample_wts /= new_sample_wts.sum()

        if x+1 < iters:
            self.sample_wts[x+1] = new_sample_wts

        self.stumps[x] = stump
        self.stump_wts[x] = stump_wt
        self.errors[x] = error
        key.append(x)

       
    plt.plot(key,self.errors)
    plt.xlabel("No. of Iterations")
    plt.ylabel("Weighted Error")
    plt.show()

    return self

def predict(self, X):
    stump_predictions = np.array([stump.predict(X) for stump in self.stumps])
    return np.sign(np.dot(self.stump_wts, stump_predictions))

y_train

AdaBoost.fit = fit
AdaBoost.predict = predict

clf = AdaBoost().fit(x_train, y_train, iters=200)

train_err = (clf.predict(x_test) != y_test).mean()
print(f'Train error: {train_err:.1%}')

td = pd.read_csv("1646102559_513849_test35-nolabels.txt", header=None)
# First 5 rows of our data
td.dropna

# from pandas.core.frame import DataFrame
tp=clf.predict(td)
tp=pd.DataFrame(tp).astype(int)

tp= tp.replace(1,5)
tp= tp.replace(-1,3)

output = open('./outputAssing2.txt', 'w')

output.write(tp.to_string(header=False, index=False))
output.close()

#graph plotting
    itr=[10,50,100,150,200,300,400,500]
    Train_error=[0.078,0.037,0.037,0.033,0.016,0.021,0.021,0.016]
    plt.plot(itr,Train_error)

    plt.xlabel("No. of Iterations")

    plt.ylabel("Train Error")

    plt.show()

#graph plotting
    itr=[10,50,100,150,200,300,400,500]
    Test_error=[0.09,0.05,0.05,0.06,0.06,0.06,0.06,0.07]
    plt.plot(itr,Test_error)

    plt.xlabel("No. of Iterations")

    plt.ylabel("Test Error")

    plt.show()

stump = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)

stump = stump.fit(x_train, y_train)

train_err = (stump.predict(x_test) != y_test).mean()

print(f'Train error: {train_err:.1%}')

Train_error=[0.078,0.037,0.037,0.033,0.016,0.021,0.021,0.016]
Test_error=[0.09,0.05,0.05,0.06,0.06,0.06,0.06,0.07]
singleDT_error=[0.082,0.082,0.082,0.082,0.082,0.082,0.082,0.082]
itr=[10,50,100,150,200,300,400,500]
plt.plot(itr, Train_error, 'g', label='Training error')
plt.plot(itr, Test_error, 'b', label='Test error')
plt.plot(itr, singleDT_error, 'r', label='Single Decision Tree error')
plt.title('Training and Test error')
plt.xlabel('No of Iterations')
plt.ylabel('Error')
plt.legend()
plt.show()